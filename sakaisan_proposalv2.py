# %%
import queue
import threading
import time

import numpy as np
import scipy.optimize as so

from benchmark_tensor_operations import TensorOperationsBenchmark

# --- è¨­å®š ---
N_LOCAL_SEARCH = 10  # ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¹ãƒ¬ãƒƒãƒ‰ã®æ•°ï¼ˆï¼ä¸¦åˆ—å®Ÿè¡Œã™ã‚‹å±€æ‰€æ¢ç´¢ã®æ•°ï¼‰
BATCH_SIZE = 5  # ãƒãƒƒãƒå‡¦ç†ã®æœ€å¤§ã‚µã‚¤ã‚º
MASTER_TIMEOUT = 0.01  # ãƒã‚¹ã‚¿ãƒ¼ãŒãƒãƒƒãƒã‚’ç¢ºå®šã™ã‚‹ã¾ã§ã®å¾…æ©Ÿæ™‚é–“ (ç§’)
N_TRIALS = 10000
DIMENSION = 100

BATCHED_TENSOR_OPS = TensorOperationsBenchmark(
    n_trials=N_TRIALS, dimension=DIMENSION, batch_size=BATCH_SIZE
)

SINGLE_TENSOR_OPS = TensorOperationsBenchmark(
    n_trials=N_TRIALS, dimension=DIMENSION, batch_size=1
)


WEIGHTS = np.random.rand(DIMENSION) * 40
WEIGHTS_batched = np.tile(WEIGHTS, (BATCH_SIZE, 1))  # shape: (BATCH_SIZE, DIMENSION)
# --- å…±æœ‰ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ ---
request_queue = queue.Queue()


def func_and_grad(x: np.ndarray) -> tuple[float, np.ndarray]:
    SINGLE_TENSOR_OPS.execute()
    x = x
    return np.sum(x**2), 2 * x


# --- è©•ä¾¡å¯¾è±¡ã®é–¢æ•°ï¼ˆãƒãƒƒãƒå¯¾å¿œç‰ˆï¼‰ ---
def func_and_grad_batched(x_batch: np.ndarray) -> tuple[np.ndarray, np.ndarray]:
    """
    (N, D)å½¢çŠ¶ã®Nå€‹ã®ç‚¹xã‚’ãƒãƒƒãƒã§å—ã‘å–ã‚Šã€
    Nå€‹ã®é–¢æ•°å€¤ã¨(N, D)å½¢çŠ¶ã®å‹¾é…ã‚’è¿”ã™
    """
    # å®Ÿéš›ã®é‡ã„è¨ˆç®—ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
    BATCHED_TENSOR_OPS.execute()
    x_batch = x_batch
    fvals = np.sum(x_batch**2, axis=1)
    grads = 2 * x_batch
    return fvals, grads


# --- ãƒã‚¹ã‚¿ãƒ¼ã‚¹ãƒ¬ãƒƒãƒ‰ã®ãƒ­ã‚¸ãƒƒã‚¯ ---
def master_thread_logic():
    """ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’åé›†ã—ã€ãƒãƒƒãƒå‡¦ç†ã‚’å®Ÿè¡Œã—ã¦çµæœã‚’è¿”ã™"""
    print("ğŸ¤– [Master]  Master thread started.")
    while True:
        x_list = []
        result_containers = []
        try:
            # ãƒãƒƒãƒã‚µã‚¤ã‚ºã«é”ã™ã‚‹ã‹ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã™ã‚‹ã¾ã§ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’åé›†
            while len(x_list) < BATCH_SIZE:
                # æœ€åˆã®1ä»¶ã¯ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãªã—ã§å¾…ã¤
                timeout = MASTER_TIMEOUT if x_list else None
                req, res_q = request_queue.get(timeout=timeout)

                if req is None:  # çµ‚äº†ã‚·ã‚°ãƒŠãƒ«
                    print("ğŸ‘‹ [Master]  Master thread finished.")
                    return

                x_list.append(req)
                result_containers.append(res_q)
        except queue.Empty:
            pass  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ãŸã®ã§ç¾åœ¨ã®ãƒãƒƒãƒã§å‡¦ç†ã‚’ç¶šè¡Œ

        if x_list:
            print(f"âš™ï¸  [Master]  Batch processing {len(x_list)} items...")
            x_batch = np.vstack(x_list)
            fvals, grads = func_and_grad_batched(x_batch)

            # å„ãƒ¯ãƒ¼ã‚«ãƒ¼ã®å°‚ç”¨ã‚­ãƒ¥ãƒ¼ã«çµæœã‚’è¿”ã™
            for i, res_q in enumerate(result_containers):
                res_q.put((fvals[i], grads[i]))


# --- SciPyã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã¨éåŒæœŸå‡¦ç†ã‚’ç¹‹ããƒ©ãƒƒãƒ‘ãƒ¼ã‚¯ãƒ©ã‚¹ ---
class FuncAndGradWrapper:
    """
    fmin_l_bfgs_bã«æ¸¡ã™ãŸã‚ã®callableã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€‚
    å†…éƒ¨ã§ãƒã‚¹ã‚¿ãƒ¼ã¨ã®éåŒæœŸé€šä¿¡ã‚’è¡Œã†ã€‚
    """

    def __init__(self, req_q: queue.Queue):
        self.request_queue = req_q
        self.result_queue = queue.Queue(maxsize=1)

    def __call__(self, x: np.ndarray) -> tuple[float, np.ndarray]:
        # SciPyã‹ã‚‰å‘¼ã°ã‚ŒãŸã‚‰ã€ãƒã‚¹ã‚¿ãƒ¼ã«è©•ä¾¡ã‚’ä¾é ¼
        self.request_queue.put((x, self.result_queue))

        # ãƒã‚¹ã‚¿ãƒ¼ã‹ã‚‰ã®çµæœã‚’å¾…ã¤
        fval, grad = self.result_queue.get()
        return fval, grad


# --- ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¹ãƒ¬ãƒƒãƒ‰ã®ãƒ­ã‚¸ãƒƒã‚¯ ---
def worker_thread_logic(worker_id: int, bounds: list, results_list: list):
    """ä¸€ã¤ã®ç‹¬ç«‹ã—ãŸL-BFGS-Bæœ€é©åŒ–ã‚’å®Ÿè¡Œã™ã‚‹"""
    print(f"  ğŸ‘· [Worker {worker_id}] Started.")

    # ã“ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ï¼ˆæœ€é©åŒ–ã‚»ãƒƒã‚·ãƒ§ãƒ³ï¼‰å°‚ç”¨ã®ãƒ©ãƒƒãƒ‘ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
    wrapped_func_and_grad = FuncAndGradWrapper(request_queue)

    min_x, min_f, info = so.fmin_l_bfgs_b(
        func=wrapped_func_and_grad,
        x0=STARTING_POINTS[worker_id],
        bounds=bounds,
        iprint=-1,  # é€”ä¸­çµŒéã‚’éè¡¨ç¤º
    )

    print(f"  ğŸ‰ [Worker {worker_id}] Finished. f(x) = {min_f:.4f} ")
    results_list.append({"fval": min_f, "x": min_x, "info": info})


# %%
# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---
BOUNDS = [(-5.0, 5.0) for _ in range(DIMENSION)]
STARTING_POINTS = [
    np.random.uniform(BOUNDS[0][0], BOUNDS[0][1], size=len(BOUNDS))
    for _ in range(N_LOCAL_SEARCH)
]

final_results = []
# 1. ãƒã‚¹ã‚¿ãƒ¼ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’ãƒ‡ãƒ¼ãƒ¢ãƒ³ã¨ã—ã¦èµ·å‹•
# (ãƒ¡ã‚¤ãƒ³ã‚¹ãƒ¬ãƒƒãƒ‰ãŒçµ‚äº†ã—ãŸã‚‰è‡ªå‹•çš„ã«çµ‚äº†ã™ã‚‹)
master = threading.Thread(target=master_thread_logic, daemon=True)
master.start()
start = time.time()

# 2. ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’Nå€‹èµ·å‹•
workers = []
for i in range(N_LOCAL_SEARCH):
    # results_listã‚’æ¸¡ã—ã¦ã€å„ã‚¹ãƒ¬ãƒƒãƒ‰ã®çµæœã‚’åé›†ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹
    worker = threading.Thread(
        target=worker_thread_logic, args=(i, BOUNDS, final_results)
    )
    workers.append(worker)
    worker.start()

# 3. ã™ã¹ã¦ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã®çµ‚äº†ã‚’å¾…ã¤
for worker in workers:
    worker.join()

# 4. ãƒã‚¹ã‚¿ãƒ¼ã«çµ‚äº†ã‚·ã‚°ãƒŠãƒ«ã‚’é€ã‚‹
# (ãƒ¯ãƒ¼ã‚«ãŒå…¨ã¦çµ‚äº†ã—ãŸã®ã§ã€ã‚‚ã†ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¯æ¥ãªã„)
# ãƒ‡ãƒ¼ãƒ¢ãƒ³ã‚¹ãƒ¬ãƒƒãƒ‰ãªã®ã§å³å¯†ã«ã¯ä¸è¦ã§ã™ãŒã€æ˜ç¤ºçš„ã«çµ‚äº†ã•ã›ã¾ã™ã€‚
request_queue.put((None, None))
master.join(timeout=1)
elapsed = time.time() - start

# 5. å…¨æ¢ç´¢ã®ä¸­ã‹ã‚‰æœ€è‰¯ã®çµæœã‚’è¦‹ã¤ã‘ã‚‹
if final_results:
    best_result = min(final_results, key=lambda r: r["fval"])
    print("\n" + "=" * 50)
    print("ğŸš€ Overall Best Result of Parallel Optimization:")
    print(f"  Minimum function value: {best_result['fval']:.6f}")
    print(f"  Elapsed time: {elapsed:.2f} seconds")
    print(f"  Found at x: {best_result['x']}")
    print("=" * 50)

# %%
# Optimize Sequentially

best_fval = float("inf")
best_x = None
start = time.time()
for i in range(N_LOCAL_SEARCH):
    min_x, min_f, info = so.fmin_l_bfgs_b(
        func=func_and_grad,
        x0=STARTING_POINTS[i],
        bounds=BOUNDS,
        iprint=-1,  # é€”ä¸­çµŒéã‚’éè¡¨ç¤º
    )
    # cast np.ndarray to float for
    min_f = min_f.item()
    print(f"  ğŸ‰ [Seq {i}] Finished. f(x) = {min_f:.4f} at x = {min_x}")
    if min_f < best_fval:
        best_fval = min_f
        best_x = min_x
elapsed = time.time() - start

print("\n" + "=" * 50)
print("ğŸš€ Overall Best Result of Sequential Optimization:")
print(f"  Minimum function value: {best_fval:.6f}")
print(f"  Found at x: {best_x}")
print(f"  Elapsed time: {elapsed:.2f} seconds")
print("=" * 50)

# %%
